advanced regression
===

# old business


## return to "views" data

- code
- understanding

## proof of positivity of hessian

## BIC

- derivation
- innoculation

## limits of OLS/inversion

### alternatives

- newton
- coordinate descent
   + NB: where is the regulariation?
- gradient descent
- stochastic (and parallelism)
  + in examples
  + in features

# why do we fit?

- interpreting results

## who cares? what works?

- log posterior is a convex loss in data plus a regularizer
- [generic form](http://web.cse.ohio-state.edu/mlss09/mlss09_talks/5.june-FRI/jordan.pdf) in terms of [convex](http://en.wikipedia.org/wiki/Convex_function#Definition) functions; see also [paper](http://arxiv.org/pdf/math/0510521.pdf)
  + in this view, what is coordinate ascent?
    - regression
    - classification

- boosting
   + NB: simple loss
   + NB: where is the regulariation?
- pseudocode
- interpretations

## for later: naive bayes

- mindset
- derivation
- generalization
